"""
Test rubric_workflow.yml: RubricJudge creates nuggets, PrefNuggetJudge judges.

This tests the baseline workflow where parametric nuggets (from LLM knowledge)
are generated by RubricJudge and then used by PrefNuggetJudge for grading.
"""

import pytest
from typing import List
from unittest.mock import patch

from autojudge_base import Request, Report
from autojudge_base.report import ReportMetaData, Rag24ReportSentence
from autojudge_base.nugget_data import NuggetBanks
from minima_llm import MinimaLlmConfig

from trec25.judges.rubric.rubric_autojudge import RubricJudge, NuggetGenerationData
from trec25.judges.prefnugget.prefnugget_judge import PrefNuggetJudge


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def llm_config() -> MinimaLlmConfig:
    """Minimal LLM config for mocked tests."""
    return MinimaLlmConfig.from_env()


@pytest.fixture
def iceberg_topic() -> Request:
    """Sample topic about icebergs."""
    return Request(
        request_id="iceberg-001",
        title="Icebergs",
        problem_statement="Elaborate how icebergs melt",
        background="Concerned citizen that wants to understand the geophysical principles",
    )


@pytest.fixture
def sample_topics(iceberg_topic) -> List[Request]:
    """List of sample topics."""
    return [iceberg_topic]


@pytest.fixture
def sample_responses(iceberg_topic) -> List[Report]:
    """Create sample RAG responses for the iceberg topic."""
    responses = []

    # Run A: Good response about iceberg melting
    metadata_a = ReportMetaData(
        run_id="run-A",
        narrative_id=iceberg_topic.request_id,
        narrative=iceberg_topic.problem_statement,
        team_id="test-team",
        type="automatic",
    )
    answer_a = [
        Rag24ReportSentence(
            text="Icebergs melt through several geophysical processes including solar radiation absorption and warm water contact.",
            citations=[0],
        ),
        Rag24ReportSentence(
            text="The rate of melting depends on water temperature, salinity, and wave action.",
            citations=[1],
        ),
    ]
    responses.append(Report(
        metadata=metadata_a,
        answer=answer_a,
        references=["doc-thermal-dynamics", "doc-ocean-currents"],
    ))

    # Run B: Weaker response
    metadata_b = ReportMetaData(
        run_id="run-B",
        narrative_id=iceberg_topic.request_id,
        narrative=iceberg_topic.problem_statement,
        team_id="test-team",
        type="automatic",
    )
    answer_b = [
        Rag24ReportSentence(
            text="Icebergs are large pieces of ice that float in the ocean.",
            citations=[0],
        ),
    ]
    responses.append(Report(
        metadata=metadata_b,
        answer=answer_b,
        references=["doc-basic-ice"],
    ))

    return responses


# =============================================================================
# Mock Helpers
# =============================================================================


def mock_nugget_generation(data: List[NuggetGenerationData], *_args, **_kwargs) -> List[NuggetGenerationData]:
    """Mock run_dspy_batch_generic for nugget generation - populates questions."""
    for item in data:
        item.questions = [
            "What causes icebergs to melt?",
            "How does water temperature affect iceberg melting?",
            "What role does salinity play in iceberg melting?",
        ]
    return data


def mock_nugget_grading(data: List, *_args, **_kwargs) -> List:
    """Mock run_dspy_batch_generic for nugget grading - populates grades."""
    for item in data:
        # Assign grades based on run_id (run-A gets higher grades)
        if hasattr(item, 'run_id') and item.run_id == "run-A":
            item.grade = 4
        else:
            item.grade = 2
        item.reasoning = "Mock reasoning"
        item.confidence = 0.9
    return data


# =============================================================================
# Tests
# =============================================================================


class TestRubricWorkflow:
    """Test rubric_workflow.yml: RubricJudge nuggets + PrefNuggetJudge judging."""

    @patch('trec25.judges.rubric.rubric_autojudge.run_dspy_batch_generic')
    def test_rubric_creates_nuggets_with_prefnugget_baseline_prompt(
        self,
        mock_dspy,
        sample_topics,
        sample_responses,
        llm_config,
    ):
        """RubricJudge creates nuggets using prefnugget-baseline prompt."""
        mock_dspy.side_effect = mock_nugget_generation

        judge = RubricJudge()
        nugget_banks = judge.create_nuggets(
            rag_topics=sample_topics,
            rag_responses=sample_responses,  # Not used for parametric
            llm_config=llm_config,
            prompt="prefnugget-baseline",
        )

        # Verify nugget banks created
        assert nugget_banks is not None
        assert isinstance(nugget_banks, NuggetBanks)

        # Verify topic has nuggets
        assert "iceberg-001" in nugget_banks.banks
        bank = nugget_banks.banks["iceberg-001"]
        nuggets = bank.nuggets_as_list()
        assert len(nuggets) == 3

        # Verify question content
        questions = [n.question for n in nuggets]
        assert "What causes icebergs to melt?" in questions

    @patch('trec_auto_judge.llm.minima_llm_dspy.run_dspy_batch_generic')
    def test_prefnugget_judges_with_rubric_nuggets(
        self,
        mock_dspy,
        sample_topics,
        sample_responses,
        llm_config,
    ):
        """PrefNuggetJudge can judge using nuggets from RubricJudge."""
        mock_dspy.side_effect = mock_nugget_grading

        # Create nuggets manually (simulating RubricJudge output)
        from trec25.judges.shared.rubric_common import build_nugget_banks
        questions_by_topic = {
            "iceberg-001": ("Icebergs", [
                "What causes icebergs to melt?",
                "How does water temperature affect iceberg melting?",
            ])
        }
        nugget_banks = build_nugget_banks(questions_by_topic)

        # Run PrefNuggetJudge
        judge = PrefNuggetJudge()
        leaderboard = judge.judge(
            rag_responses=sample_responses,
            rag_topics=sample_topics,
            llm_config=llm_config,
            nugget_banks=nugget_banks,
            grade_threshold=3,
        )

        # Verify leaderboard
        assert leaderboard is not None

        # Verify both runs are present
        run_ids = {e.run_id for e in leaderboard.entries}
        assert "run-A" in run_ids
        assert "run-B" in run_ids

    @patch('trec_auto_judge.llm.minima_llm_dspy.run_dspy_batch_generic')
    @patch('trec25.judges.rubric.rubric_autojudge.run_dspy_batch_generic')
    def test_full_workflow_rubric_to_prefnugget(
        self,
        mock_rubric_dspy,
        mock_prefnugget_dspy,
        sample_topics,
        sample_responses,
        llm_config,
    ):
        """End-to-end: RubricJudge creates nuggets, PrefNuggetJudge judges."""
        mock_rubric_dspy.side_effect = mock_nugget_generation
        mock_prefnugget_dspy.side_effect = mock_nugget_grading

        # Step 1: RubricJudge creates nuggets
        rubric_judge = RubricJudge()
        nugget_banks = rubric_judge.create_nuggets(
            rag_topics=sample_topics,
            rag_responses=sample_responses,
            llm_config=llm_config,
            prompt="prefnugget-baseline",
        )

        assert nugget_banks is not None

        # Step 2: PrefNuggetJudge uses those nuggets
        prefnugget_judge = PrefNuggetJudge()
        leaderboard = prefnugget_judge.judge(
            rag_responses=sample_responses,
            rag_topics=sample_topics,
            llm_config=llm_config,
            nugget_banks=nugget_banks,
            grade_threshold=3,
        )

        assert leaderboard is not None

        # Verify run-A scored higher (it had grade 4, run-B had grade 2)
        # With threshold=3, run-A should have higher coverage
        # Find coverage values from leaderboard entries (use "all" topic for aggregates)
        run_a_entries = [e for e in leaderboard.entries if e.run_id == "run-A" and e.topic_id == "all"]
        run_b_entries = [e for e in leaderboard.entries if e.run_id == "run-B" and e.topic_id == "all"]

        assert len(run_a_entries) > 0, "No 'all' entry for run-A"
        assert len(run_b_entries) > 0, "No 'all' entry for run-B"

        run_a_coverage = run_a_entries[0].values.get("NUGGET_COVERAGE", 0)
        run_b_coverage = run_b_entries[0].values.get("NUGGET_COVERAGE", 0)
        assert run_a_coverage > run_b_coverage