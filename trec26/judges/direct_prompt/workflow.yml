# Direct Prompt Judge Workflow Configuration

# Judge class for dynamic loading
judge_class: "trec26.judges.direct_prompt.direct_prompt_judge.DirectPromptJudge"

# Lifecycle: create qrels â†’ judge
create_nuggets: false
create_qrels: true   # LLM grades passages first using prompts
judge: true          # Then aggregates to leaderboard

# Data flow
judge_uses_qrels: true   # Judge needs qrels from create_qrels()
judge_uses_nuggets: false
qrels_uses_nuggets: false

# Settings
settings:
  filebase: "{_name}"

# Qrels creation settings
# NOTE: topic_format MUST be explicitly set - no auto-detection
# Use --dataset flag with run_judge.py or manually set here
qrels_settings:
  topic_format: REQUIRED  # MUST be: rag, ragtime, or dragun
  filebase: "{_name}"

# Judge settings
# NOTE: topic_format controls query extraction and prompt selection
judge_settings:
  topic_format: REQUIRED  # MUST be: rag, ragtime, or dragun
  relevance_threshold: 2  # Grade >= 2 is "relevant"
  filebase: "{_name}"

# Variants for different thresholds
variants:
  strict:
    judge_settings:
      relevance_threshold: 3  # Only grade 3 is relevant

  lenient:
    judge_settings:
      relevance_threshold: 1  # Grade >= 1 is relevant
